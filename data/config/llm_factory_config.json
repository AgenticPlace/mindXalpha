{
  "default_provider_preference_order": [
    "gemini",
    "groq",
    "ollama",
    "anthropic",
    "openai",
    "mock"
  ],
  "ollama_settings_for_factory": {
    "default_model_override": null,
    "base_url_override": null,
    "cloud_run_friendly_models": {
        "excellent_fit": [
            {"id": "phi3_json_cfg", "ollama_name": "phi3:mini-instruct-4k-fp16", "type": "general/reasoning", "est_memory_needed_gb": 4, "notes": "From JSON: Microsoft Phi-3 Mini Instruct (fp16 for balance)."},
            {"id": "gemma2b_json_cfg", "ollama_name": "gemma:2b-instruct-fp16", "type": "general", "est_memory_needed_gb": 3, "notes": "From JSON: Google's 2B open model (fp16)."},
            {"id": "codegemma2b_json_cfg", "ollama_name": "codegemma:2b", "type": "coding", "est_memory_needed_gb": 3, "notes": "From JSON: Google's 2B code model."}
        ],
        "good_fit": [
            {"id": "codegemma7b_json_cfg", "ollama_name": "codegemma:7b-it", "type": "coding", "est_memory_needed_gb": 8, "notes": "From JSON: Google's 7B code model, instruction tuned."}
        ],
        "default_coding_preference_order": ["codegemma7b_json_cfg", "codegemma2b_json_cfg", "phi3_json_cfg"],
        "default_general_preference_order": ["phi3_json_cfg", "gemma2b_json_cfg", "llama3_8b_py"]
    }
  },
  "gemini_settings_for_factory": {
    "default_model_override": "gemini-1.5-flash-latest",
    "api_key_override": null
  },
  "groq_settings_for_factory": {
    "default_model_override": "llama3-8b-8192",
    "api_key_override": null
  },
  "provider_specific_handler_config": {
    "ollama": {
        "default_model_for_api_call": "nous-hermes2:latest"
    },
    "gemini": {
        "default_model_for_api_call": "gemini-1.5-flash-latest"
    },
    "groq": {
        "default_model_for_api_call": "llama3-8b-8192"
    }
  }
}
